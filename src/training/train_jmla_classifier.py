#!/usr/bin/env python3
"""
Train JMLA (OpenJMLA) Classifier on FMA Dataset
Extracts OpenJMLA embeddings and trains a simple MLP classifier

Expected Accuracy: 70-80%
Training Time: 2-4 hours
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import librosa
from pathlib import Path
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# Genre mapping (16 FMA genres)
GENRES = ['Blues', 'Classical', 'Country', 'Easy Listening', 'Electronic', 
          'Experimental', 'Folk', 'Hip-Hop', 'Instrumental', 'International',
          'Jazz', 'Old-Time/Historic', 'Pop', 'Rock', 'Soul-RnB', 'Spoken']

class SimpleClassifier(nn.Module):
    """Simple MLP classifier"""
    def __init__(self, input_dim=768, num_classes=16):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes)
        )
    
    def forward(self, x):
        return self.fc(x)

class JMLAFeatureExtractor:
    """Extract OpenJMLA embeddings from audio files"""
    def __init__(self, device='cuda'):
        self.device = device
        print("Loading OpenJMLA model...")
        # TODO: Implement OpenJMLA model loading
        # For now, this is a placeholder
        print("Note: OpenJMLA feature extraction not yet implemented")
        print("Using dummy features for demonstration")
        self.model = None
    
    def extract(self, audio_path):
        """Extract 768-dim JMLA embedding"""
        # TODO: Implement actual OpenJMLA feature extraction
        # For now, return dummy features
        audio, sr = librosa.load(audio_path, sr=16000, duration=30)
        
        # Placeholder: Extract mel-spectrogram features
        mel = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)
        mel_db = librosa.power_to_db(mel, ref=np.max)
        
        # Dummy 768-dim embedding (replace with actual OpenJMLA)
        embedding = np.mean(mel_db, axis=1)
        embedding = np.pad(embedding, (0, 768-len(embedding)), 'constant')
        
        return embedding[:768]

def extract_jmla_features(fma_audio_dir, fma_metadata, output_path, device='cuda'):
    """Extract JMLA features for all FMA tracks"""
    extractor = JMLAFeatureExtractor(device)
    
    # Load FMA metadata
    metadata = np.load(fma_metadata, allow_pickle=True).item()
    
    features = {}
    print(f"\nExtracting JMLA features from {len(metadata)} tracks...")
    
    for track_id, info in tqdm(metadata.items()):
        audio_path = Path(fma_audio_dir) / info['path']
        if audio_path.exists():
            try:
                features[track_id] = {
                    'embedding': extractor.extract(str(audio_path)),
                    'genre': info['genre'],
                    'genre_id': GENRES.index(info['genre'])
                }
            except Exception as e:
                print(f"Error processing {track_id}: {e}")
    
    # Save features
    np.save(output_path, features)
    print(f"\nSaved {len(features)} JMLA features to {output_path}")
    return features

def train_classifier(features_path, output_model_path, device='cuda', epochs=20, batch_size=32):
    """Train classifier on JMLA features"""
    
    # Load features
    print(f"Loading JMLA features from {features_path}...")
    features = np.load(features_path, allow_pickle=True).item()
    
    # Prepare data
    X = np.array([f['embedding'] for f in features.values()])
    y = np.array([f['genre_id'] for f in features.values()])
    
    # Split train/val
    from sklearn.model_selection import train_test_split
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Convert to tensors
    X_train = torch.FloatTensor(X_train).to(device)
    y_train = torch.LongTensor(y_train).to(device)
    X_val = torch.FloatTensor(X_val).to(device)
    y_val = torch.LongTensor(y_val).to(device)
    
    # Create model
    model = SimpleClassifier(768, 16).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    print(f"\nTraining on {len(X_train)} samples, validating on {len(X_val)} samples")
    print(f"Device: {device}")
    
    best_acc = 0
    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0
        train_correct = 0
        
        for i in range(0, len(X_train), batch_size):
            batch_X = X_train[i:i+batch_size]
            batch_y = y_train[i:i+batch_size]
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            train_correct += (predicted == batch_y).sum().item()
        
        train_acc = 100 * train_correct / len(X_train)
        
        # Validation
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val)
            val_loss = criterion(val_outputs, y_val)
            _, val_predicted = torch.max(val_outputs, 1)
            val_correct = (val_predicted == y_val).sum().item()
            val_acc = 100 * val_correct / len(X_val)
        
        print(f"Epoch {epoch+1}/{epochs}: "
              f"Train Loss: {train_loss/len(X_train)*batch_size:.4f}, "
              f"Train Acc: {train_acc:.2f}%, "
              f"Val Loss: {val_loss:.4f}, "
              f"Val Acc: {val_acc:.2f}%")
        
        # Save best model
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), output_model_path)
            print(f"  â†’ Saved best model (Val Acc: {val_acc:.2f}%)")
    
    print(f"\nTraining complete! Best validation accuracy: {best_acc:.2f}%")
    print(f"Model saved to: {output_model_path}")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Train JMLA Classifier')
    parser.add_argument('--mode', choices=['extract', 'train', 'both'], default='both',
                       help='Extract features, train classifier, or both')
    parser.add_argument('--fma-audio', 
                       default='/media/mijesu_970/SSD_Data/DataSets/FMA/Data/fma_medium',
                       help='FMA audio directory')
    parser.add_argument('--fma-metadata',
                       default='/media/mijesu_970/SSD_Data/AI_models/FMA/FMA.npy',
                       help='FMA metadata file')
    parser.add_argument('--features-output',
                       default='/media/mijesu_970/SSD_Data/AI_models/FMA/JMLA_features.npy',
                       help='Output path for JMLA features')
    parser.add_argument('--model-output',
                       default='/media/mijesu_970/SSD_Data/AI_models/trained_models/jmla_classifier.pth',
                       help='Output path for trained model')
    parser.add_argument('--device', default='cuda', choices=['cuda', 'cpu'])
    parser.add_argument('--epochs', type=int, default=20)
    parser.add_argument('--batch-size', type=int, default=32)
    
    args = parser.parse_args()
    
    if args.mode in ['extract', 'both']:
        print("="*60)
        print("STEP 1: Extracting JMLA Features")
        print("="*60)
        extract_jmla_features(
            args.fma_audio,
            args.fma_metadata,
            args.features_output,
            args.device
        )
    
    if args.mode in ['train', 'both']:
        print("\n" + "="*60)
        print("STEP 2: Training JMLA Classifier")
        print("="*60)
        train_classifier(
            args.features_output,
            args.model_output,
            args.device,
            args.epochs,
            args.batch_size
        )

if __name__ == '__main__':
    main()
